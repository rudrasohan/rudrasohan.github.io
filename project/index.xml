<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Personal Website {Work in Progress}</title>
    <link>https://rudrasohan.github.io/project/</link>
      <atom:link href="https://rudrasohan.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://rudrasohan.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://rudrasohan.github.io/project/</link>
    </image>
    
    <item>
      <title>MADRaS</title>
      <link>https://rudrasohan.github.io/project/madras/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://rudrasohan.github.io/project/madras/</guid>
      <description>&lt;p&gt;Car-Racing is a sport which requires ultra-precise reflexes along with the understanding of harmony between the driver and the race-car. In essence, it can provide a challenging testbed for testing the limits of an autonomous driving agent. Building upon this philosophy and also observing the budding popularity of TORCS in RL community, we created MADRaS which can provide a proper environment for Multi-Agent Reinforcement Learning.&lt;/p&gt;
&lt;p&gt;MADRaS provide an OpenAI Gym interface for the TORCS environment along with support for parallelism. A user can create their custom environment selecting from a plethora of TORCS tracks and vehicles using our interface which melds this into a python API. The entire simulator can easily be configured with a single file. Additionally, we also bring support for randomized environment creation, custom control schemes and custom traffic agents to help create diverse scenarios.&lt;/p&gt;
&lt;p&gt;The main crux of MADRaS boils down to its Multi-Agent capabilities. The entire Multi-Agent system has been designed following the guidelines provided by the 
&lt;a href=&#34;https://bair.berkeley.edu/blog/2018/12/12/rllib/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BAIR&lt;/a&gt; lab at UC Berkely. Each agent can interact separately with the simulator and with the other agents. Each agent has a communication overhead, and the entire communication network can be reconfigured by a single configuration file.&lt;/p&gt;
&lt;p&gt;The entire project is open-sourced under the AGPL-3.0 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Frenet Planner</title>
      <link>https://rudrasohan.github.io/project/frenet-planner/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://rudrasohan.github.io/project/frenet-planner/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://www.researchgate.net/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frenet PLanner&lt;/a&gt; is a motion planning algorithm for car-like vehicles. For a given state of a car the planner samples posssible next states in the Frenet Frame and connects them. The algorithm uses simple splines for connecting various the points.&lt;/p&gt;
&lt;p&gt;The splines are computed using Kinematic equations of velocities and positions with respect to time. The paths are composed of two different spliens longitudinal and transversal. The longitudinal splines are characterized by velocities where as the transversal splines are characterized by position. After the relvant splines are computed a routine filters out the splines which are not following the kinematic and saftey constraits. Finally the filtered paths are evaluated on a certain objective function which minimizes jerk in tranjectory and the most optimal one is selected for execution.&lt;/p&gt;
&lt;p&gt;Finally we transfer the path to the path tracker algorithm which generates required velocities for the low level controller.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Safe RL</title>
      <link>https://rudrasohan.github.io/project/safe-reinforcement-learning/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://rudrasohan.github.io/project/safe-reinforcement-learning/</guid>
      <description>&lt;p&gt;Reinforcement Learning is a tool which helps an artificial agent to &amp;ldquo;Learn by Doing&amp;rdquo;. The agents interact with the system and learn the ropes becoming better in each iteration. Most of RL research focusses on training agents in simulations as during early iterations of learning some of the agent&amp;rsquo;s actions may be harmful to itself or the user or its surroundings. But many-a-times it&amp;rsquo;s not possible to train in simulation owing to either difficulty in the representation of the environment virtually or due to the sheer gap in generalization performance. Hence, in such cases, Safe-Reinforcement learning algorithms help train the agents considering the system constraints.&lt;/p&gt;
&lt;p&gt;There are several ways to enforce AI Saftey. In this work, we have selected two aspects of safety, Performance Bonds and Constraint Optimization.&lt;/p&gt;
&lt;p&gt;One aspect of AI-Saftey could be described in terms of algorithmic performance, i.e. the actions of an agent are considered safe until the overall performance lies above a certain threshold.  Using techniques like Importance Sampling one can estimate the agent&amp;rsquo;s policy performance before it is executed.&lt;/p&gt;
&lt;p&gt;The other way to enforce safety is by making sure the algorithm makes updates which satisfy certain constraints. The vast literature on constraint optimization boasts several techniques which ensure that the policy transitions only from one safe state into the next. For these methods, one needs to set up functional forms for each of the constraints.&lt;/p&gt;
&lt;p&gt;We see that these techniques ameliorate the issues with safety-aware training.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
